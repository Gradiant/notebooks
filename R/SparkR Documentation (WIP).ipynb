{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# SparkR tutorial Notebook\n",
    "This is a notebook version of the [SparkR Documentation](http://spark.apache.org/docs/2.4.0/sparkr.html)\n",
    "## Overview\n",
    "SparkR is an R package that provides a light-weight frontend to use Apache Spark from R. In Spark 2.4.0, SparkR provides a distributed data frame implementation that supports operations like selection, filtering, aggregation etc. (similar to R data frames, dplyr) but on large datasets. SparkR also supports distributed machine learning using MLlib.\n",
    "\n",
    "# SparkDataFrame\n",
    "A SparkDataFrame is a distributed collection of data organized into named columns. It is conceptually equivalent to a table in a relational database or a data frame in R, but with richer optimizations under the hood. SparkDataFrames can be constructed from a wide array of sources such as: structured data files, tables in Hive, external databases, or existing local R data frames.\n",
    "\n",
    "All of the examples on this page use sample data included in R or the Spark distribution and can be run using the ./bin/sparkR shell.\n",
    "\n",
    "## Starting Up: SparkSession\n",
    "Y\n",
    "ou can start SparkR from Jupyter R Kernel. You can connect your R program to a Spark cluster from RStudio, R shell, Rscript or other R IDEs. To start, make sure SPARK_HOME is set in environment (you can check Sys.getenv), load the SparkR package, and call sparkR.session as below. It will check for the Spark installation, and, if not found, it will be downloaded and cached automatically. Alternatively, you can also run install.spark manually.\n",
    "\n",
    "In addition to calling sparkR.session, you could also specify certain Spark driver properties. Normally these Application properties and Runtime Environment cannot be set programmatically, as the driver JVM process would have been started, in this case SparkR takes care of this for you. To set them, pass them as you would other configuration properties in the sparkConfig argument to sparkR.session()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Java ref type org.apache.spark.sql.SparkSession id 1 "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if (nchar(Sys.getenv(\"SPARK_HOME\")) < 1) {\n",
    "  Sys.setenv(SPARK_HOME = \"/home/spark\")\n",
    "}\n",
    "library(SparkR, lib.loc = c(file.path(Sys.getenv(\"SPARK_HOME\"), \"R\", \"lib\")))\n",
    "sparkR.session(master = \"local[*]\", sparkConfig = list(spark.driver.memory = \"2g\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following Spark driver properties can be set in sparkConfig with sparkR.session from RStudio:\n",
    "\n",
    "Property Name | Property group | spark-submit equivalent\n",
    " --- | --- | --- \n",
    "spark.master |\tApplication Properties\t| --master\n",
    "spark.yarn.keytab|Application Properties|\t--keytab\n",
    "spark.yarn.principal|\tApplication Properties|\t--principal\n",
    "spark.driver.memory|\tApplication Properties|\t--driver-memory\n",
    "spark.driver.extraClassPath|\tRuntime Environment|\t--driver-class-path\n",
    "spark.driver.extraJavaOptions|\tRuntime Environment|\t--driver-java-options\n",
    "spark.driver.extraLibraryPath|\tRuntime Environment|\t--driver-library-path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating SparkDataFrames\n",
    "With a SparkSession, applications can create SparkDataFrames from a local R data frame, from a Hive table, or from other data sources.\n",
    "\n",
    "### From local data frames\n",
    "The simplest way to create a data frame is to convert a local R data frame into a SparkDataFrame. Specifically, we can use as.DataFrame or createDataFrame and pass in the local R data frame to create a SparkDataFrame. As an example, the following creates a SparkDataFrame based using the faithful dataset from R."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th scope=col>eruptions</th><th scope=col>waiting</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><td>3.600</td><td>79   </td></tr>\n",
       "\t<tr><td>1.800</td><td>54   </td></tr>\n",
       "\t<tr><td>3.333</td><td>74   </td></tr>\n",
       "\t<tr><td>2.283</td><td>62   </td></tr>\n",
       "\t<tr><td>4.533</td><td>85   </td></tr>\n",
       "\t<tr><td>2.883</td><td>55   </td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|ll}\n",
       " eruptions & waiting\\\\\n",
       "\\hline\n",
       "\t 3.600 & 79   \\\\\n",
       "\t 1.800 & 54   \\\\\n",
       "\t 3.333 & 74   \\\\\n",
       "\t 2.283 & 62   \\\\\n",
       "\t 4.533 & 85   \\\\\n",
       "\t 2.883 & 55   \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "eruptions | waiting | \n",
       "|---|---|---|---|---|---|\n",
       "| 3.600 | 79    | \n",
       "| 1.800 | 54    | \n",
       "| 3.333 | 74    | \n",
       "| 2.283 | 62    | \n",
       "| 4.533 | 85    | \n",
       "| 2.883 | 55    | \n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "  eruptions waiting\n",
       "1 3.600     79     \n",
       "2 1.800     54     \n",
       "3 3.333     74     \n",
       "4 2.283     62     \n",
       "5 4.533     85     \n",
       "6 2.883     55     "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df <- as.DataFrame(faithful)\n",
    "\n",
    "# Displays the first part of the SparkDataFrame\n",
    "head(df)\n",
    "##  eruptions waiting\n",
    "##1     3.600      79\n",
    "##2     1.800      54\n",
    "##3     3.333      74"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From Data Sources\n",
    "SparkR supports operating on a variety of data sources through the SparkDataFrame interface. This section describes the general methods for loading and saving data using Data Sources. You can check the Spark SQL programming guide for more specific options that are available for the built-in data sources.\n",
    "\n",
    "The general method for creating SparkDataFrames from data sources is read.df. This method takes in the path for the file to load and the type of data source, and the currently active SparkSession will be used automatically. SparkR supports reading JSON, CSV and Parquet files natively, and through packages available from sources like Third Party Projects, you can find data source connectors for popular file formats like Avro. These packages can either be added by specifying --packages with spark-submit or sparkR commands, or if initializing SparkSession with sparkPackages parameter when in an interactive R shell or from RStudio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Java ref type org.apache.spark.sql.SparkSession id 1 "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sparkR.session(sparkPackages = \"com.databricks:spark-avro_2.11:3.0.0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see how to use data sources using an example JSON input file. Note that the file that is used here is not a typical JSON file. Each line in the file must contain a separate, self-contained valid JSON object. For more information, please see JSON Lines text format, also called newline-delimited JSON. As a consequence, a regular multi-line JSON file will most often fail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th scope=col>age</th><th scope=col>name</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><td>NA     </td><td>Michael</td></tr>\n",
       "\t<tr><td>30     </td><td>Andy   </td></tr>\n",
       "\t<tr><td>19     </td><td>Justin </td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|ll}\n",
       " age & name\\\\\n",
       "\\hline\n",
       "\t NA      & Michael\\\\\n",
       "\t 30      & Andy   \\\\\n",
       "\t 19      & Justin \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "age | name | \n",
       "|---|---|---|\n",
       "| NA      | Michael | \n",
       "| 30      | Andy    | \n",
       "| 19      | Justin  | \n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "  age name   \n",
       "1 NA  Michael\n",
       "2 30  Andy   \n",
       "3 19  Justin "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "people <- read.df(\"/opt/spark/examples/src/main/resources/people.json\", \"json\")\n",
    "head(people)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SparkR automatically infers the schema from the JSON file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- age: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n"
     ]
    }
   ],
   "source": [
    "printSchema(people)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, multiple files can be read with read.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "people <- read.json(c(\"./examples/src/main/resources/people.json\", \"./examples/src/main/resources/people.json\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data sources API natively supports CSV formatted input files. For more information please refer to SparkR [read.df](http://spark.apache.org/docs/latest/api/R/read.df.html) API documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "csvPath <- \"/opt/spark/examples/src/main/resources/people.csv\"\n",
    "df <- read.df(csvPath, \"csv\", header = \"true\", inferSchema = \"true\", na.strings = \"NA\", delimiter=\";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th scope=col>name</th><th scope=col>age</th><th scope=col>job</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><td>Jorge    </td><td>30       </td><td>Developer</td></tr>\n",
       "\t<tr><td>Bob      </td><td>32       </td><td>Developer</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|lll}\n",
       " name & age & job\\\\\n",
       "\\hline\n",
       "\t Jorge     & 30        & Developer\\\\\n",
       "\t Bob       & 32        & Developer\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "name | age | job | \n",
       "|---|---|\n",
       "| Jorge     | 30        | Developer | \n",
       "| Bob       | 32        | Developer | \n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "  name  age job      \n",
       "1 Jorge 30  Developer\n",
       "2 Bob   32  Developer"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "head(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SparkDataFrame Operations\n",
    "SparkDataFrames support a number of functions to do structured data processing. Here we include some basic examples and a complete list can be found in the API docs:\n",
    "\n",
    "## Selecting rows, columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SparkDataFrame[eruptions:double, waiting:double]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th scope=col>eruptions</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><td>3.600</td></tr>\n",
       "\t<tr><td>1.800</td></tr>\n",
       "\t<tr><td>3.333</td></tr>\n",
       "\t<tr><td>2.283</td></tr>\n",
       "\t<tr><td>4.533</td></tr>\n",
       "\t<tr><td>2.883</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|l}\n",
       " eruptions\\\\\n",
       "\\hline\n",
       "\t 3.600\\\\\n",
       "\t 1.800\\\\\n",
       "\t 3.333\\\\\n",
       "\t 2.283\\\\\n",
       "\t 4.533\\\\\n",
       "\t 2.883\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "eruptions | \n",
       "|---|---|---|---|---|---|\n",
       "| 3.600 | \n",
       "| 1.800 | \n",
       "| 3.333 | \n",
       "| 2.283 | \n",
       "| 4.533 | \n",
       "| 2.883 | \n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "  eruptions\n",
       "1 3.600    \n",
       "2 1.800    \n",
       "3 3.333    \n",
       "4 2.283    \n",
       "5 4.533    \n",
       "6 2.883    "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th scope=col>eruptions</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><td>3.600</td></tr>\n",
       "\t<tr><td>1.800</td></tr>\n",
       "\t<tr><td>3.333</td></tr>\n",
       "\t<tr><td>2.283</td></tr>\n",
       "\t<tr><td>4.533</td></tr>\n",
       "\t<tr><td>2.883</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|l}\n",
       " eruptions\\\\\n",
       "\\hline\n",
       "\t 3.600\\\\\n",
       "\t 1.800\\\\\n",
       "\t 3.333\\\\\n",
       "\t 2.283\\\\\n",
       "\t 4.533\\\\\n",
       "\t 2.883\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "eruptions | \n",
       "|---|---|---|---|---|---|\n",
       "| 3.600 | \n",
       "| 1.800 | \n",
       "| 3.333 | \n",
       "| 2.283 | \n",
       "| 4.533 | \n",
       "| 2.883 | \n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "  eruptions\n",
       "1 3.600    \n",
       "2 1.800    \n",
       "3 3.333    \n",
       "4 2.283    \n",
       "5 4.533    \n",
       "6 2.883    "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th scope=col>eruptions</th><th scope=col>waiting</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><td>1.750</td><td>47   </td></tr>\n",
       "\t<tr><td>1.750</td><td>47   </td></tr>\n",
       "\t<tr><td>1.867</td><td>48   </td></tr>\n",
       "\t<tr><td>1.750</td><td>48   </td></tr>\n",
       "\t<tr><td>2.167</td><td>48   </td></tr>\n",
       "\t<tr><td>2.100</td><td>49   </td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|ll}\n",
       " eruptions & waiting\\\\\n",
       "\\hline\n",
       "\t 1.750 & 47   \\\\\n",
       "\t 1.750 & 47   \\\\\n",
       "\t 1.867 & 48   \\\\\n",
       "\t 1.750 & 48   \\\\\n",
       "\t 2.167 & 48   \\\\\n",
       "\t 2.100 & 49   \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "eruptions | waiting | \n",
       "|---|---|---|---|---|---|\n",
       "| 1.750 | 47    | \n",
       "| 1.750 | 47    | \n",
       "| 1.867 | 48    | \n",
       "| 1.750 | 48    | \n",
       "| 2.167 | 48    | \n",
       "| 2.100 | 49    | \n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "  eruptions waiting\n",
       "1 1.750     47     \n",
       "2 1.750     47     \n",
       "3 1.867     48     \n",
       "4 1.750     48     \n",
       "5 2.167     48     \n",
       "6 2.100     49     "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df <- as.DataFrame(faithful)\n",
    "\n",
    "# Get basic information about the SparkDataFrame\n",
    "df\n",
    "## SparkDataFrame[eruptions:double, waiting:double]\n",
    "\n",
    "# Select only the \"eruptions\" column\n",
    "head(select(df, df$eruptions))\n",
    "##  eruptions\n",
    "##1     3.600\n",
    "##2     1.800\n",
    "##3     3.333\n",
    "\n",
    "# You can also pass in column name as strings\n",
    "head(select(df, \"eruptions\"))\n",
    "\n",
    "# Filter the SparkDataFrame to only retain rows with wait times shorter than 50 mins\n",
    "head(filter(df, df$waiting < 50))\n",
    "##  eruptions waiting\n",
    "##1     1.750      47\n",
    "##2     1.750      47\n",
    "##3     1.867      48"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data type mapping between R and Spark\n",
    "\n",
    "| R | Spark |\n",
    "| --- | --- |\n",
    "|byte|byte|\n",
    "|integer\t|integer|\n",
    "|float\t|float|\n",
    "|double\t|double|\n",
    "|numeric\t|double|\n",
    "|character\t|string|\n",
    "|string\t|string|\n",
    "|binary\t|binary|\n",
    "|raw\t|binary|\n",
    "|logical\t|boolean|\n",
    "|POSIXct\t|timestamp|\n",
    "|POSIXlt\t|timestamp|\n",
    "|Date\t|date|\n",
    "|array\t|array|\n",
    "|list\t|array|\n",
    "|env\t|map|\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.5.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": false,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
