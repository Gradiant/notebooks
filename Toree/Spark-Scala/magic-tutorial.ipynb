{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Toree Magics<a name=\"top\"></a>\n",
    "Magics are special \"functions\" which enable features or execute some special code. Magics can receive input arguments when they are invoked. There are two types of magics: `cell` magics and `line` magics. Magics invocations are not case sensitive.\n",
    "\n",
    "**Table of Contents**\n",
    "\n",
    "1. [Line Magics](#line-magics)\n",
    "    1. [LsMagic](#lsmagic)\n",
    "    1. [Truncation](#truncation)\n",
    "    1. [ShowTypes](#showtypes)\n",
    "    1. [AddJar](#addjar)\n",
    "    1. [AddDeps](#adddeps)\n",
    "1. [Cell Magics](#cell-magics)\n",
    "    1. [DataFrame](#dataframe)\n",
    "    1. [Html](#html)\n",
    "    1. [JavaScript](#javascript)\n",
    "    1. [PySpark](#pyspark)\n",
    "    1. [SparkR](#sparkr)\n",
    "    1. [SparkSQL](#sparksql)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Line Magics<a name=\"line-magics\"></a><span style=\"float: right; font-size: 0.5em\"><a href=\"#top\">Top</a></span>\n",
    "Line magics are run on a single line and can have other code and line magics within the same cell. Line magics use the following syntax: \n",
    "\n",
    "```\n",
    "%magicname [args]\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### %LsMagic<a name=\"lsmagic\"></a><span style=\"float: right; font-size: 0.5em\"><a href=\"#top\">Top</a></span>\n",
    "The `LsMagic` is a magic to list all the available magics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available line magics:\n",
      "%lsmagic %showtypes %adddeps %truncation %addjar\n",
      "\n",
      "Available cell magics:\n",
      "%%sql %%html %%javascript %%dataframe %%pyspark %%scala %%sparkr\n",
      "\n",
      "Type %<magic_name> for usage info.\n",
      "         \n"
     ]
    }
   ],
   "source": [
    "%LsMagic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### %Truncation<a name=\"truncation\"></a><span style=\"float: right; font-size: 0.5em\"><a href=\"#top\">Top</a></span>\n",
    "Toree will, by default, truncate results from statements. This can be managed through the `%Truncation` magic. To see the current state of the truncation setting you can invoke the magic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Truncation is currently on \n"
     ]
    }
   ],
   "source": [
    "// invoke the truncation magic to see if truncation is on or off\n",
    "%Truncation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Range(1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// return a value to see the truncation\n",
    "(1 to 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output will NOT be truncated\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Range(1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433, 434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447, 448, 449, 450, 451, 452, 453, 454, 455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473, 474, 475, 476, 477, 478, 479, 480, 481, 482, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493, 494, 495, 496, 497, 498, 499, 500, 501, 502, 503, 504, 505, 506, 507, 508, 509, 510, 511, 512, ... )"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%Truncation off\n",
    "(1 to 2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output WILL be truncated.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Range(1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433, 434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447, 448, 449, 450, 451, 452, 453, 454, 455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473, 474, 475, 476, 477, 478, 479, 480, 481, 482, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493, 494, 495, 496, 497, 498, 499, 500, 501, 502, 503, 504, 505, 506, 507, 508, 509, 510, 511, 512, ... )"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%Truncation on\n",
    "(1 to 2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### %ShowTypes<a name=\"showtypes\"></a><span style=\"float: right; font-size: 0.5em\"><a href=\"#top\">Top</a></span>\n",
    "The type information for a result is hidden by default. This behavior can be changed by using the `%ShowTypes` magic. You can view the current state of `%ShowTypes` by invoking it with no arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ShowTypes is currently off \n"
     ]
    }
   ],
   "source": [
    "%ShowTypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Hello types!"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"Hello types!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Types will be printed.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Hello types!"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%ShowTypes on\n",
    "\"Hello types!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1,Hello types!)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(1, \"Hello types!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Types will not be printed\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Hello types!"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%ShowTypes off\n",
    "\"Hello types!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### %AddJar<a name=\"addjar\"></a><span style=\"float: right; font-size: 0.5em\"><a href=\"#top\">Top</a></span>\n",
    "`AddJar` is a magic that allows the addition of jars to Torree's environment. You can see the arguments for `AddJar` by invoking it with no arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usage: %AddJar <jar_url>\n",
      "\n",
      "Option   Description                        \n",
      "------   -----------                        \n",
      "-f       forces re-download of specified jar\n",
      "--magic  loads jar as a magic extension     \n",
      "Option   Description                        \n",
      "------   -----------                        \n",
      "-f       forces re-download of specified jar\n",
      "--magic  loads jar as a magic extension     \n"
     ]
    }
   ],
   "source": [
    "%AddJar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting download from https://repo1.maven.org/maven2/org/lwjgl/lwjgl/3.0.0b/lwjgl-3.0.0b.jar\n",
      "Finished download of lwjgl-3.0.0b.jar\n"
     ]
    }
   ],
   "source": [
    "%AddJar https://repo1.maven.org/maven2/org/lwjgl/lwjgl/3.0.0b/lwjgl-3.0.0b.jar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.0.0b SNAPSHOT"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "org.lwjgl.Version.getVersion()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## %AddDeps<a name=\"adddeps\"></a><span style=\"float: right; font-size: 0.5em\"><a href=\"#top\">Top</a></span>\n",
    "`AddDeps` is a magic to add dependencies from a maven repository. You can see the arguments for `AddDeps` by invoking it with no arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usage: %AddDeps my.company artifact-id version\n",
      "\n",
      "Option                        Description                          \n",
      "------                        -----------                          \n",
      "--abort-on-resolution-errors  Abort (no downloads) when resolution \n",
      "                                fails                              \n",
      "--classifier                  Sets the dependency's classifier     \n",
      "--credential                  Adds a credential file to be used to \n",
      "                                the list                           \n",
      "--exclude                     exclude dependency                   \n",
      "--ivy-configuration           Sets the Ivy configuration for the   \n",
      "                                dependency; defaults to \"default\"  \n",
      "--repository                  Adds an additional repository to     \n",
      "                                available list                     \n",
      "--trace                       Prints out trace of download progress\n",
      "--transitive                  Retrieve dependencies recursively    \n",
      "--verbose                     Prints out additional information    \n"
     ]
    }
   ],
   "source": [
    "%AddDeps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note, that by default the `AddDeps` magic will only retrieve the specified dependency. If you want the transitive dependencies provide the `--transitive` flag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Marking org.joda:joda-money:0.11 for download\n",
      "Preparing to fetch from:\n",
      "-> file:/tmp/toree-tmp-dir3110415039222417243/toree_add_deps/\n",
      "-> https://repo1.maven.org/maven2\n",
      "=> 1 (): Downloading https://repo1.maven.org/maven2/org/joda/joda-money/0.11/joda-money-0.11.pom\n",
      "=> 1 (): Downloading https://repo1.maven.org/maven2/org/joda/joda-money/0.11/joda-money-0.11.pom.sha1\n",
      "===> 1 (joda-money-0.11.pom): Is 22792 total bytes\n",
      "===> 1 (joda-money-0.11.pom): Downloaded 2235 bytes (9.81%)\n",
      "===> 1 (joda-money-0.11.pom): Downloaded 4994 bytes (21.91%)\n",
      "===> 1 (joda-money-0.11.pom): Downloaded 7753 bytes (34.02%)\n",
      "===> 1 (joda-money-0.11.pom): Downloaded 10512 bytes (46.12%)\n",
      "===> 1 (joda-money-0.11.pom): Downloaded 13271 bytes (58.23%)\n",
      "===> 1 (joda-money-0.11.pom): Downloaded 15860 bytes (69.59%)\n",
      "===> 1 (joda-money-0.11.pom): Downloaded 18619 bytes (81.69%)\n",
      "===> 1 (joda-money-0.11.pom): Downloaded 21378 bytes (93.80%)\n",
      "===> 1 (joda-money-0.11.pom): Downloaded 22792 bytes (100.00%)\n",
      "===> 1 (joda-money-0.11.pom.sha1): Is 40 total bytes\n",
      "===> 1 (joda-money-0.11.pom.sha1): Downloaded 40 bytes (100.00%)\n",
      "=> 1 (joda-money-0.11.pom): Finished downloading\n",
      "=> 1 (joda-money-0.11.pom.sha1): Finished downloading\n",
      "=> 2 (): Downloading https://repo1.maven.org/maven2/org/joda/joda-money/0.11/\n",
      "===> 2 (.directory): Is 2936 total bytes\n",
      "===> 2 (.directory): Downloaded 2346 bytes (79.90%)\n",
      "===> 2 (.directory): Downloaded 2936 bytes (100.00%)\n",
      "=> 2 (.directory): Finished downloading\n",
      "=> 2 (): Downloading https://repo1.maven.org/maven2/org/joda/joda-money/0.11/joda-money-0.11.jar\n",
      "=> 1 (): Downloading https://repo1.maven.org/maven2/org/joda/joda-money/0.11/joda-money-0.11.jar.sha1\n",
      "=> https://repo1.maven.org/maven2/org/joda/joda-money/0.11/joda-money-0.11.pom: Found at /tmp/toree-tmp-dir3110415039222417243/toree_add_deps/https/repo1.maven.org/maven2/org/joda/joda-money/0.11/joda-money-0.11.pom\n",
      "=> https://repo1.maven.org/maven2/org/joda/joda-money/0.11/joda-money-0.11.pom.sha1: Found at /tmp/toree-tmp-dir3110415039222417243/toree_add_deps/https/repo1.maven.org/maven2/org/joda/joda-money/0.11/joda-money-0.11.pom.sha1\n",
      "===> 2 (joda-money-0.11.jar): Is 63725 total bytes\n",
      "===> 1 (joda-money-0.11.jar.sha1): Is 40 total bytes\n",
      "===> 2 (joda-money-0.11.jar): Downloaded 2219 bytes (3.48%)\n",
      "===> 1 (joda-money-0.11.jar.sha1): Downloaded 40 bytes (100.00%)\n",
      "===> 2 (joda-money-0.11.jar): Downloaded 4978 bytes (7.81%)\n",
      "===> 2 (joda-money-0.11.jar): Downloaded 7737 bytes (12.14%)\n",
      "=> 1 (joda-money-0.11.jar.sha1): Finished downloading\n",
      "===> 2 (joda-money-0.11.jar): Downloaded 10496 bytes (16.47%)\n",
      "===> 2 (joda-money-0.11.jar): Downloaded 13255 bytes (20.80%)\n",
      "===> 2 (joda-money-0.11.jar): Downloaded 15844 bytes (24.86%)\n",
      "===> 2 (joda-money-0.11.jar): Downloaded 18603 bytes (29.19%)\n",
      "===> 2 (joda-money-0.11.jar): Downloaded 21362 bytes (33.52%)\n",
      "===> 2 (joda-money-0.11.jar): Downloaded 24121 bytes (37.85%)\n",
      "===> 2 (joda-money-0.11.jar): Downloaded 26880 bytes (42.18%)\n",
      "===> 2 (joda-money-0.11.jar): Downloaded 29639 bytes (46.51%)\n",
      "===> 2 (joda-money-0.11.jar): Downloaded 32228 bytes (50.57%)\n",
      "===> 2 (joda-money-0.11.jar): Downloaded 48612 bytes (76.28%)\n",
      "===> 2 (joda-money-0.11.jar): Downloaded 63725 bytes (100.00%)\n",
      "=> 2 (joda-money-0.11.jar): Finished downloading\n",
      "-> New file at /tmp/toree-tmp-dir3110415039222417243/toree_add_deps/https/repo1.maven.org/maven2/org/joda/joda-money/0.11/joda-money-0.11.jar\n",
      "-> New file at /tmp/toree-tmp-dir3110415039222417243/toree_add_deps/https/repo1.maven.org/maven2/org/joda/joda-money/0.11/joda-money-0.11.pom\n"
     ]
    }
   ],
   "source": [
    "%AddDeps org.joda joda-money 0.11 --transitive --trace --verbose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AUD"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "org.joda.money.CurrencyUnit.AUD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Cell Magics<a name=\"cell-magics\"></a><span style=\"float: right; font-size: 0.5em\"><a href=\"#top\">Top</a></span>\n",
    "Cell magics are magics which take the whole cell as their argument. They take the following form:\n",
    "\n",
    "```\n",
    "%%magicname\n",
    "line1\n",
    "line2\n",
    "...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### %%DataFrame<a name=\"dataframe\"></a><span style=\"float: right; font-size: 0.5em\"><a href=\"#top\">Top</a></span>\n",
    "The `%%DataFrame` magic is used to convert a Spark SQL DataFrame into various formats. Currently, `json`, `html`, and `csv` are supported. The magic takes an expression, which evauluates to a dataframe, to perform the conversion. So, we first need to create a DataFrame object for reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined class DFRecord\n",
       "sqlc = org.apache.spark.sql.SparkSession@3e040e2e\n",
       "df = [key: string, value: int]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "error: error while loading QualifiedTableName, class file '/opt/spark-2.3.2-bin-hadoop2.7/jars/spark-catalyst_2.11-2.3.2.jar(org/apache/spark/sql/catalyst/QualifiedTableName.class)' has location not matching its contents: contains class QualifiedTableName\n",
       "error: error while loading JavaTypeInference, class file '/opt/spark-2.3.2-bin-hadoop2.7/jars/spark-catalyst_2.11-2.3.2.jar(org/apache/spark/sql/catalyst/JavaTypeInference.class)' has location not matching its contents: contains class JavaTypeInference\n",
       "error: error while loading FunctionIdentifier, class file '/opt/spark-2.3.2-bin-hadoop2.7/jars/spark-catalyst_2.11-2.3.2.jar(org/apache/spark/sql/catalyst/FunctionIdentifier.class)' has location not matching its contents: contains class FunctionIdentifier\n",
       "error: error while loading DefinedByConstructorParams, class file '/opt/spark-2.3.2-bin-hadoop2.7/jars/spark-catalyst_2.11-2.3.2.jar(org/apache/spark/sql/catalyst/DefinedByConstructorParams.class)' has location not matching its contents: contains class DefinedByConstructorParams\n",
       "error: error while loading IdentifierWithDatabase, class file '/opt/spark-2.3.2-bin-hadoop2.7/jars/spark-catalyst_2.11-2.3.2.jar(org/apache/spark/sql/catalyst/IdentifierWithDatabase.class)' has location not matching its contents: contains class IdentifierWithDatabase\n",
       "error: error while loading SpecializedGetters, class file '/opt/spark-2.3.2-bin-hadoop2.7/jars/spark-catalyst_2.11-2.3.2.jar(org/apache/spark/sql/catalyst/expressions/SpecializedGetters.class)' has location not matching its contents: contains class SpecializedGetters\n",
       "error: error while loading Decimal, class file '/opt/spark-2.3.2-bin-hadoop2.7/jars/spark-catalyst_2.11-2.3.2.jar(org/apache/spark/sql/types/Decimal.class)' has location not matching its contents: contains class Decimal\n",
       "error: error while loading ScalaReflection, class file '/opt/spark-2.3.2-bin-hadoop2.7/jars/spark-catalyst_2.11-2.3.2.jar(org/apache/spark/sql/catalyst/ScalaReflection.class)' has location not matching its contents: contains trait ScalaReflection\n",
       "error: error while loading CatalystTypeConverters, class file '/opt/spark-2.3.2-bin-hadoop2.7/jars/spark-catalyst_2.11-2.3.2.jar(org/apache/spark/sql/catalyst/CatalystTypeConverters.class)' has location not matching its contents: contains class CatalystTypeConverters\n",
       "error: error while loading package, class file '/opt/spark-2.3.2-bin-hadoop2.7/jars/spark-catalyst_2.11-2.3.2.jar(org/apache/spark/sql/catalyst/dsl/package.class)' has location not matching its contents: contains package object dsl\n",
       "error: error while loading package, class file '/opt/spark-2.3.2-bin-hadoop2.7/jars/spark-catalyst_2.11-2.3.2.jar(org/apache/spark/sql/catalyst/util/package.class)' has location not matching its contents: contains package object util\n",
       "error: error while loading package, class file '/opt/spark-2.3.2-bin-hadoop2.7/jars/spark-catalyst_2.11-2.3.2.jar(org/apache/spark/sql/catalyst/errors/package.class)' has location not matching its contents: contains package object errors\n",
       "error: error while loading KVIterator, class file '/opt/spark-2.3.2-bin-hadoop2.7/jars/spark-unsafe_2.11-2.3.2.jar(org/apache/spark/unsafe/KVIterator.class)' has location not matching its contents: contains class KVIterator\n",
       "error: error while loading UnsafeAlignedOffset, class file '/opt/spark-2.3.2-bin-hadoop2.7/jars/spark-unsafe_2.11-2.3.2.jar(org/apache/spark/unsafe/UnsafeAlignedOffset.class)' has location not matching its contents: contains class UnsafeAlignedOffset\n",
       "error: error while loading Platform, class file '/opt/spark-2.3.2-bin-hadoop2.7/jars/spark-unsafe_2.11-2.3.2.jar(org/apache/spark/unsafe/Platform.class)' has location not matching its contents: contains class Platform\n",
       "error: error while loading CalendarInterval, class file '/opt/spark-2.3.2-bin-hadoop2.7/jars/spark-unsafe_2.11-2.3.2.jar(org/apache/spark/unsafe/types/CalendarInterval.class)' has location not matching its contents: contains class CalendarInterval\n",
       "error: error while loading UTF8String, class file '/opt/spark-2.3.2-bin-hadoop2.7/jars/spark-unsafe_2.11-2.3.2.jar(org/apache/spark/unsafe/types/UTF8String.class)' has location not matching its contents: contains class UTF8String\n",
       "error: error while loading ByteArray, class file '/opt/spark-2.3.2-bin-hadoop2.7/jars/spark-unsafe_2.11-2.3.2.jar(org/apache/spark/unsafe/types/ByteArray.class)' has location not matching its contents: contains class ByteArray\n",
       "error: error while loading ByteType, class file '/opt/spark-2.3.2-bin-hadoop2.7/jars/spark-catalyst_2.11-2.3.2.jar(org/apache/spark/sql/types/ByteType.class)' has location not matching its contents: contains class ByteType\n",
       "error: error while loading PythonUserDefinedType, class file '/opt/spark-2.3.2-bin-hadoop2.7/jars/spark-catalyst_2.11-2.3.2.jar(org/apache/spark/sql/types/PythonUserDefinedType.class)' has location not matching its contents: contains class PythonUserDefinedType\n",
       "error: error while loading Metadata, class file '/opt/spark-2.3.2-bin-hadoop2.7/jars/spark-catalyst_2.11-2.3.2.jar(org/apache/spark/sql/types/Metadata.class)' has location not matching its contents: contains class Metadata\n",
       "error: error while loading UserDefinedType, class file '/opt/spark-2.3.2-bin-hadoop2.7/jars/spark-catalyst_2.11-2.3.2.jar(org/apache/spark/sql/types/UserDefinedType.class)' has location not matching its contents: contains class UserDefinedType\n",
       "error: error while loading LongType, class file '/opt/spark-2.3.2-bin-hadoop2.7/jars/spark-catalyst_2.11-2.3.2.jar(org/apache/spark/sql/types/LongType.class)' has location not matching its contents: contains class LongType\n",
       "error: error while loading DataTypes, class file '/opt/spark-2.3.2-bin-hadoop2.7/jars/spark-catalyst_2.11-2.3.2.jar(org/apache/spark/sql/types/DataTypes.class)' has location not matching its contents: contains class DataTypes\n",
       "error: error while loading NumericType, class file '/opt/spark-2.3.2-bin-hadoop2.7/jars/spark-catalyst_2.11-2.3.2.jar(org/apache/spark/sql/types/NumericType.class)' has location not matching its contents: contains class NumericType\n",
       "error: error while loading StructField, class file '/opt/spark-2.3.2-bin-hadoop2.7/jars/spark-catalyst_2.11-2.3.2.jar(org/apache/spark/sql/types/StructField.class)' has location not matching its contents: contains class StructField\n",
       "error: error while loading MetadataBuilder, class file '/opt/spark-2.3.2-bin-hadoop2.7/jars/spark-catalyst_2.11-2.3.2.jar(org/apache/spark/sql/types/MetadataBuilder.class)' has location not matching its contents: contains class MetadataBuilder\n",
       "error: error while loading FractionalType, class file '/opt/spark-2.3.2-bin-hadoop2.7/jars/spark-catalyst_2.11-2.3.2.jar(org/apache/spark/sql/types/FractionalType.class)' has location not matching its contents: contains class FractionalType\n",
       "error: error while loading TypeCollection, class file '/opt/spark-2.3.2-bin-hadoop2.7/jars/spark-catalyst_2.11-2.3.2.jar(org/apache/spark/sql/types/TypeCollection.class)' has location not matching its contents: contains class TypeCollection\n",
       "error: error while loading DoubleType, class file '/opt/spark-2.3.2-bin-hadoop2.7/jars/spark-catalyst_2.11-2.3.2.jar(org/apache/spark/sql/types/DoubleType.class)' has location not matching its contents: contains class DoubleType\n",
       "error: error while loading AnyDataType, class file '/opt/spark-2.3.2-bin-hadoop2.7/jars/spark-catalyst_2.11-2.3.2.jar(org/apache/spark/sql/types/AnyDataType.class)' has location not matching its contents: contains class AnyDataType\n",
       "error: error while loading StringType, class file '/opt/spark-2.3.2-bin-hadoop2.7/jars/spark-catalyst_2.11-2.3.2.jar(org/apache/spark/sql/types/StringType.class)' has location not matching its contents: contains class StringType\n",
       "error: error while loading ObjectType, class file '/opt/spark-2.3.2-bin-hadoop2.7/jars/spark-catalyst_2.11-2.3.2.jar(org/apache/spark/sql/types/ObjectType.class)' has location not matching its contents: contains class ObjectType\n",
       "error: error while loading NullType, class file '/opt/spark-2.3.2-bin-hadoop2.7/jars/spark-catalyst_2.11-2.3.2.jar(org/apache/spark/sql/types/NullType.class)' has location not matching its contents: contains class NullType\n",
       "error: error while loading ShortType, class file '/opt/spark-2.3.2-bin-hadoop2.7/jars/spark-catalyst_2.11-2.3.2.jar(org/apache/spark/sql/types/ShortType.class)' has location not matching its contents: contains class ShortType\n",
       "error: error while loading IntegralType, class file '/opt/spark-2.3.2-bin-hadoop2.7/jars/spark-catalyst_2.11-2.3.2.jar(org/apache/spark/sql/types/IntegralType.class)' has location not matching its contents: contains class IntegralType\n",
       "error: error while loading AbstractDataType, class file '/opt/spark-2.3.2-bin-hadoop2.7/jars/spark-catalyst_2.11-2.3.2.jar(org/apache/spark/sql/types/AbstractDataType.class)' has location not matching its contents: contains class AbstractDataType\n",
       "error: error while loading Expression, class file '/opt/spark-2.3.2-bin-hadoop2.7/jars/spark-catalyst_2.11-2.3.2.jar(org/apache/spark/sql/catalyst/expressions/Expression.class)' has location not matching its contents: contains class Expression\n",
       "error: error while loading InterpretedOrdering, class file '/opt/spark-2.3.2-bin-hadoop2.7/jars/spark-catalyst_2.11-2.3.2.jar(org/apache/spark/sql/catalyst/expressions/InterpretedOrdering.class)' has location not matching its contents: contains class InterpretedOrdering\n",
       "error: error while loading HiveStringType, class file '/opt/spark-2.3.2-bin-hadoop2.7/jars/spark-catalyst_2.11-2.3.2.jar(org/apache/spark/sql/types/HiveStringType.class)' has location not matching its contents: contains class HiveStringType\n",
       "error: error while loading ArrayType, class file '/opt/spark-2.3.2-bin-hadoop2.7/jars/spark-catalyst_2.11-2.3.2.jar(org/apache/spark/sql/types/ArrayType.class)' has location not matching its contents: contains class ArrayType\n",
       "error: error while loading BinaryType, class file '/opt/spark-2.3.2-bin-hadoop2.7/jars/spark-catalyst_2.11-2.3.2.jar(org/apache/spark/sql/types/BinaryType.class)' has location not matching its contents: contains class BinaryType\n",
       "error: error while loading TimestampType, class file '/opt/spark-2.3.2-bin-hadoop2.7/jars/spark-catalyst_2.11-2.3.2.jar(org/apache/spark/sql/types/TimestampType.class)' has location not matching its contents: contains class TimestampType\n",
       "error: error while loading BooleanType, class file '/opt/spark-2.3.2-bin-hadoop2.7/jars/spark-catalyst_2.11-2.3.2.jar(org/apache/spark/sql/types/BooleanType.class)' has location not matching its contents: contains class BooleanType\n",
       "error: error while loading IntegerType, class file '/opt/spark-2.3.2-bin-hadoop2.7/jars/spark-catalyst_2.11-2.3.2.jar(org/apache/spark/sql/types/IntegerType.class)' has location not matching its contents: contains class IntegerType\n",
       "error: error while loading DecimalType, class file '/opt/spark-2.3.2-bin-hadoop2.7/jars/spark-catalyst_2.11-2.3.2.jar(org/apache/spark/sql/types/DecimalType.class)' has location not matching its contents: contains class DecimalType\n",
       "error: error while loading SQLUserDefinedType, class file '/opt/spark-2.3.2-bin-hadoop2.7/jars/spark-catalyst_2.11-2.3.2.jar(org/apache/spark/sql/types/SQLUserDefinedType.class)' has location not matching its contents: contains class SQLUserDefinedType\n",
       "error: error while loading CalendarIntervalType, class file '/opt/spark-2.3.2-bin-hadoop2.7/jars/spark-catalyst_2.11-2.3.2.jar(org/apache/spark/sql/types/CalendarIntervalType.class)' has location not matching its contents: contains class CalendarIntervalType\n",
       "error: error while loading FloatType, class file '/opt/spark-2.3.2-bin-hadoop2.7/jars/spark-catalyst_2.11-2.3.2.jar(org/apache/spark/sql/types/FloatType.class)' has location not matching its contents: contains class FloatType\n",
       "error: error while loading CharType, class file '/opt/spark-2.3.2-bin-hadoop2.7/jars/spark-catalyst_2.11-2.3.2.jar(org/apache/spark/sql/types/CharType.class)' has location not matching its contents: contains class CharType\n",
       "error: error while loading MapType, class file '/opt/spark-2.3.2-bin-hadoop2.7/jars/spark-catalyst_2.11-2.3.2.jar(org/apache/spark/sql/types/MapType.class)' has location not matching its contents: contains class MapType\n",
       "error: error while loading DateType, class file '/opt/spark-2.3.2-bin-hadoop2.7/jars/spark-catalyst_2.11-2.3.2.jar(org/apache/spark/sql/types/DateType.class)' has location not matching its contents: contains class DateType\n",
       "error: error while loading AtomicType, class file '/opt/spark-2.3.2-bin-hadoop2.7/jars/spark-catalyst_2.11-2.3.2.jar(org/apache/spark/sql/types/AtomicType.class)' has location not matching its contents: contains class AtomicType\n",
       "error: error while loading VarcharType, class file '/opt/spark-2.3.2-bin-hadoop2.7/jars/spark-catalyst_2.11-2.3.2.jar(org/apache/spark/sql/types/VarcharType.class)' has location not matching its contents: contains class VarcharType\n",
       "error: error while loading UDTRegistration, class file '/opt/spark-2.3.2-bin-hadoop2.7/jars/spark-catalyst_2.11-2.3.2.jar(org/apache/spark/sql/types/UDTRegistration.class)' has location not matching its contents: contains class UDTRegistration\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[key: string, value: int]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "case class DFRecord(key: String, value: Int)\n",
    "val sqlc = spark\n",
    "import sqlc.implicits._\n",
    "val df = sc.parallelize(1 to 10).map(x => DFRecord(x.toString, x)).toDF()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The default output is `html`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "error: error while loading Cross, class file '/opt/spark-2.3.2-bin-hadoop2.7/jars/spark-catalyst_2.11-2.3.2.jar(org/apache/spark/sql/catalyst/plans/Cross.class)' has location not matching its contents: contains class Cross\n",
       "error: error while loading NaturalJoin, class file '/opt/spark-2.3.2-bin-hadoop2.7/jars/spark-catalyst_2.11-2.3.2.jar(org/apache/spark/sql/catalyst/plans/NaturalJoin.class)' has location not matching its contents: contains class NaturalJoin\n",
       "error: error while loading LeftExistence, class file '/opt/spark-2.3.2-bin-hadoop2.7/jars/spark-catalyst_2.11-2.3.2.jar(org/apache/spark/sql/catalyst/plans/LeftExistence.class)' has location not matching its contents: contains class LeftExistence\n",
       "error: error while loading RightOuter, class file '/opt/spark-2.3.2-bin-hadoop2.7/jars/spark-catalyst_2.11-2.3.2.jar(org/apache/spark/sql/catalyst/plans/RightOuter.class)' has location not matching its contents: contains class RightOuter\n",
       "error: error while loading LeftOuter, class file '/opt/spark-2.3.2-bin-hadoop2.7/jars/spark-catalyst_2.11-2.3.2.jar(org/apache/spark/sql/catalyst/plans/LeftOuter.class)' has location not matching its contents: contains class LeftOuter\n",
       "error: error while loading ExistenceJoin, class file '/opt/spark-2.3.2-bin-hadoop2.7/jars/spark-catalyst_2.11-2.3.2.jar(org/apache/spark/sql/catalyst/plans/ExistenceJoin.class)' has location not matching its contents: contains class ExistenceJoin\n",
       "error: error while loading InnerLike, class file '/opt/spark-2.3.2-bin-hadoop2.7/jars/spark-catalyst_2.11-2.3.2.jar(org/apache/spark/sql/catalyst/plans/InnerLike.class)' has location not matching its contents: contains class InnerLike\n",
       "error: error while loading UsingJoin, class file '/opt/spark-2.3.2-bin-hadoop2.7/jars/spark-catalyst_2.11-2.3.2.jar(org/apache/spark/sql/catalyst/plans/UsingJoin.class)' has location not matching its contents: contains class UsingJoin\n",
       "error: error while loading JoinType, class file '/opt/spark-2.3.2-bin-hadoop2.7/jars/spark-catalyst_2.11-2.3.2.jar(org/apache/spark/sql/catalyst/plans/JoinType.class)' has location not matching its contents: contains class JoinType\n",
       "error: error while loading FullOuter, class file '/opt/spark-2.3.2-bin-hadoop2.7/jars/spark-catalyst_2.11-2.3.2.jar(org/apache/spark/sql/catalyst/plans/FullOuter.class)' has location not matching its contents: contains class FullOuter\n",
       "error: error while loading LeftAnti, class file '/opt/spark-2.3.2-bin-hadoop2.7/jars/spark-catalyst_2.11-2.3.2.jar(org/apache/spark/sql/catalyst/plans/LeftAnti.class)' has location not matching its contents: contains class LeftAnti\n",
       "error: error while loading LeftSemi, class file '/opt/spark-2.3.2-bin-hadoop2.7/jars/spark-catalyst_2.11-2.3.2.jar(org/apache/spark/sql/catalyst/plans/LeftSemi.class)' has location not matching its contents: contains class LeftSemi\n",
       "error: error while loading Inner, class file '/opt/spark-2.3.2-bin-hadoop2.7/jars/spark-catalyst_2.11-2.3.2.jar(org/apache/spark/sql/catalyst/plans/Inner.class)' has location not matching its contents: contains class Inner\n",
       "error: error while loading QueryPlan, class file '/opt/spark-2.3.2-bin-hadoop2.7/jars/spark-catalyst_2.11-2.3.2.jar(org/apache/spark/sql/catalyst/plans/QueryPlan.class)' has location not matching its contents: contains class QueryPlan\n",
       "error: error while loading HintInfo, class file '/opt/spark-2.3.2-bin-hadoop2.7/jars/spark-catalyst_2.11-2.3.2.jar(org/apache/spark/sql/catalyst/plans/logical/HintInfo.class)' has location not matching its contents: contains class HintInfo\n",
       "error: error while loading ColumnStat, class file '/opt/spark-2.3.2-bin-hadoop2.7/jars/spark-catalyst_2.11-2.3.2.jar(org/apache/spark/sql/catalyst/plans/logical/ColumnStat.class)' has location not matching its contents: contains class ColumnStat\n",
       "error: error while loading ScriptInputOutputSchema, class file '/opt/spark-2.3.2-bin-hadoop2.7/jars/spark-catalyst_2.11-2.3.2.jar(org/apache/spark/sql/catalyst/plans/logical/ScriptInputOutputSchema.class)' has location not matching its contents: contains class ScriptInputOutputSchema\n",
       "error: error while loading InsertIntoDir, class file '/opt/spark-2.3.2-bin-hadoop2.7/jars/spark-catalyst_2.11-2.3.2.jar(org/apache/spark/sql/catalyst/plans/logical/InsertIntoDir.class)' has location not matching its contents: contains class InsertIntoDir\n",
       "error: error while loading LogicalPlan, class file '/opt/spark-2.3.2-bin-hadoop2.7/jars/spark-catalyst_2.11-2.3.2.jar(org/apache/spark/sql/catalyst/plans/logical/LogicalPlan.class)' has location not matching its contents: contains class LogicalPlan\n",
       "error: error while loading Join, class file '/opt/spark-2.3.2-bin-hadoop2.7/jars/spark-catalyst_2.11-2.3.2.jar(org/apache/spark/sql/catalyst/plans/logical/Join.class)' has location not matching its contents: contains class Join\n",
       "error: error while loading FlatMapGroupsInPandas, class file '/opt/spark-2.3.2-bin-hadoop2.7/jars/spark-catalyst_2.11-2.3.2.jar(org/apache/spark/sql/catalyst/plans/logical/FlatMapGroupsInPandas.class)' has location not matching its contents: contains class FlatMapGroupsInPandas\n",
       "error: error while loading LogicalGroupState, class file '/opt/spark-2.3.2-bin-hadoop2.7/jars/spark-catalyst_2.11-2.3.2.jar(org/apache/spark/sql/catalyst/plans/logical/LogicalGroupState.class)' has location not matching its contents: contains class LogicalGroupState\n",
       "error: error while loading Pivot, class file '/opt/spark-2.3.2-bin-hadoop2.7/jars/spark-catalyst_2.11-2.3.2.jar(org/apache/spark/sql/catalyst/plans/logical/Pivot.class)' has location not matching its contents: contains class Pivot\n",
       "error: error while loading Generate, class file '/opt/spark-2.3.2-bin-hadoop2.7/jars/spark-catalyst_2.11-2.3.2.jar(org/apache/spark/sql/catalyst/plans/logical/Generate.class)' has location not matching its contents: contains class Generate\n",
       "error: error while loading AppendColumns, class file '/opt/spark-2.3.2-bin-hadoop2.7/jars/spark-catalyst_2.11-2.3.2.jar(org/apache/spark/sql/catalyst/plans/logical/AppendColumns.class)' has location not matching its contents: contains class AppendColumns\n",
       "error: error while loading HistogramSerializer, class file '/opt/spark-2.3.2-bin-hadoop2.7/jars/spark-catalyst_2.11-2.3.2.jar(org/apache/spark/sql/catalyst/plans/logical/HistogramSerializer.class)' has location not matching its contents: contains class HistogramSerializer\n",
       "error: error while loading Distinct, class file '/opt/spark-2.3.2-bin-hadoop2.7/jars/spark-catalyst_2.11-2.3.2.jar(org/apache/spark/sql/catalyst/plans/logical/Distinct.class)' has location not matching its contents: contains class Distinct\n",
       "error: error while loading Statistics, class file '/opt/spark-2.3.2-bin-hadoop2.7/jars/spark-catalyst_2.11-2.3.2.jar(org/apache/spark/sql/catalyst/plans/logical/Statistics.class)' has location not matching its contents: contains class Statistics\n",
       "error: error while loading SubqueryAlias, class file '/opt/spark-2.3.2-bin-hadoop2.7/jars/spark-catalyst_2.11-2.3.2.jar(org/apache/spark/sql/catalyst/plans/logical/SubqueryAlias.class)' has location not matching its contents: contains class SubqueryAlias\n",
       "error: error while loading ObjectConsumer, class file '/opt/spark-2.3.2-bin-hadoop2.7/jars/spark-catalyst_2.11-2.3.2.jar(org/apache/spark/sql/catalyst/plans/logical/ObjectConsumer.class)' has location not matching its contents: contains class ObjectConsumer\n",
       "error: error while loading ProcessingTimeTimeout, class file '/opt/spark-2.3.2-bin-hadoop2.7/jars/spark-catalyst_2.11-2.3.2.jar(org/apache/spark/sql/catalyst/plans/logical/ProcessingTimeTimeout.class)' has location not matching its contents: contains class ProcessingTimeTimeout\n",
       "error: error while loading InsertIntoTable, class file '/opt/spark-2.3.2-bin-hadoop2.7/jars/spark-catalyst_2.11-2.3.2.jar(org/apache/spark/sql/catalyst/plans/logical/InsertIntoTable.class)' has location not matching its contents: contains class InsertIntoTable\n",
       "error: error while loading MapGroups, class file '/opt/spark-2.3.2-bin-hadoop2.7/jars/spark-catalyst_2.11-2.3.2.jar(org/apache/spark/sql/catalyst/plans/logical/MapGroups.class)' has location not matching its contents: contains class MapGroups\n",
       "error: error while loading Window, class file '/opt/spark-2.3.2-bin-hadoop2.7/jars/spark-catalyst_2.11-2.3.2.jar(org/apache/spark/sql/catalyst/plans/logical/Window.class)' has location not matching its contents: contains class Window\n",
       "error: error while loading DeserializeToObject, class file '/opt/spark-2.3.2-bin-hadoop2.7/jars/spark-catalyst_2.11-2.3.2.jar(org/apache/spark/sql/catalyst/plans/logical/DeserializeToObject.class)' has location not matching its contents: contains class DeserializeToObject\n",
       "error: error while loading EventTimeTimeout, class file '/opt/spark-2.3.2-bin-hadoop2.7/jars/spark-catalyst_2.11-2.3.2.jar(org/apache/spark/sql/catalyst/plans/logical/EventTimeTimeout.class)' has location not matching its contents: contains class EventTimeTimeout\n",
       "error: error while loading MapPartitionsInR, class file '/opt/spark-2.3.2-bin-hadoop2.7/jars/spark-catalyst_2.11-2.3.2.jar(org/apache/spark/sql/catalyst/plans/logical/MapPartitionsInR.class)' has location not matching its contents: contains class MapPartitionsInR\n",
       "error: error while loading EventTimeWatermark, class file '/opt/spark-2.3.2-bin-hadoop2.7/jars/spark-catalyst_2.11-2.3.2.jar(org/apache/spark/sql/catalyst/plans/logical/EventTimeWatermark.class)' has location not matching its contents: contains class EventTimeWatermark\n",
       "error: error while loading Sample, class file '/opt/spark-2.3.2-bin-hadoop2.7/jars/spark-catalyst_2.11-2.3.2.jar(org/apache/spark/sql/catalyst/plans/logical/Sample.class)' has location not matching its contents: contains class Sample\n",
       "error: error while loading Filter, class file '/opt/spark-2.3.2-bin-hadoop2.7/jars/spark-catalyst_2.11-2.3.2.jar(org/apache/spark/sql/catalyst/plans/logical/Filter.class)' has location not matching its contents: contains class Filter\n",
       "error: error while loading AppendColumnsWithObject, class file '/opt/spark-2.3.2-bin-hadoop2.7/jars/spark-catalyst_2.11-2.3.2.jar(org/apache/spark/sql/catalyst/plans/logical/AppendColumnsWithObject.class)' has location not matching its contents: contains class AppendColumnsWithObject\n",
       "error: error while loading TypedFilter, class file '/opt/spark-2.3.2-bin-hadoop2.7/jars/spark-catalyst_2.11-2.3.2.jar(org/apache/spark/sql/catalyst/plans/logical/TypedFilter.class)' has location not matching its contents: contains class TypedFilter\n",
       "error: error while loading Histogram, class file '/opt/spark-2.3.2-bin-hadoop2.7/jars/spark-catalyst_2.11-2.3.2.jar(org/apache/spark/sql/catalyst/plans/logical/Histogram.class)' has location not matching its contents: contains class Histogram\n",
       "error: error while loading AnalysisBarrier, class file '/opt/spark-2.3.2-bin-hadoop2.7/jars/spark-catalyst_2.11-2.3.2.jar(org/apache/spark/sql/catalyst/plans/logical/AnalysisBarrier.class)' has location not matching its contents: contains class AnalysisBarrier\n",
       "error: error while loading With, class file '/opt/spark-2.3.2-bin-hadoop2.7/jars/spark-catalyst_2.11-2.3.2.jar(org/apache/spark/sql/catalyst/plans/logical/With.class)' has location not matching its contents: contains class With\n",
       "error: error while loading MapPartitions, class file '/opt/spark-2.3.2-bin-hadoop2.7/jars/spark-catalyst_2.11-2.3.2.jar(org/apache/spark/sql/catalyst/plans/logical/MapPartitions.class)' has location not matching its contents: contains class MapPartitions\n",
       "error: error while loading ResolvedHint, class file '/opt/spark-2.3.2-bin-hadoop2.7/jars/spark-catalyst_2.11-2.3.2.jar(org/apache/spark/sql/catalyst/plans/logical/ResolvedHint.class)' has location not matching its contents: contains class ResolvedHint\n",
       "error: error while loading LocalLimit, class file '/opt/spark-2.3.2-bin-hadoop2.7/jars/spark-catalyst_2.11-2.3.2.jar(org/apache/spark/sql/catalyst/plans/logical/LocalLimit.class)' has location not matching its contents: contains class LocalLimit\n",
       "error: error while loading UnresolvedHint, class file '/opt/spark-2.3.2-bin-hadoop2.7/jars/spark-catalyst_2.11-2.3.2.jar(org/apache/spark/sql/catalyst/plans/logical/UnresolvedHint.class)' has location not matching its contents: contains class UnresolvedHint\n",
       "error: error while loading Intersect, class file '/opt/spark-2.3.2-bin-hadoop2.7/jars/spark-catalyst_2.11-2.3.2.jar(org/apache/spark/sql/catalyst/plans/logical/Intersect.class)' has location not matching its contents: contains class Intersect\n",
       "error: error while loading OneRowRelation, class file '/opt/spark-2.3.2-bin-hadoop2.7/jars/spark-catalyst_2.11-2.3.2.jar(org/apache/spark/sql/catalyst/plans/logical/OneRowRelation.class)' has location not matching its contents: contains class OneRowRelation\n",
       "error: error while loading SetOperation, class file '/opt/spark-2.3.2-bin-hadoop2.7/jars/spark-catalyst_2.11-2.3.2.jar(org/apache/spark/sql/catalyst/plans/logical/SetOperation.class)' has location not matching its contents: contains class SetOperation\n",
       "error: error while loading HistogramBin, class file '/opt/spark-2.3.2-bin-hadoop2.7/jars/spark-catalyst_2.11-2.3.2.jar(org/apache/spark/sql/catalyst/plans/logical/HistogramBin.class)' has location not matching its contents: contains class HistogramBin\n",
       "error: error while loading ScriptTransformation, class file '/opt/spark-2.3.2-bin-hadoop2.7/jars/spark-catalyst_2.11-2.3.2.jar(org/apache/spark/sql/catalyst/plans/logical/ScriptTransformation.class)' has location not matching its contents: contains class ScriptTransformation\n",
       "error: error while loading RepartitionByExpression, class file '/opt/spark-2.3.2-bin-hadoop2.7/jars/spark-catalyst_2.11-2.3.2.jar(org/apache/spark/sql/catalyst/plans/logical/RepartitionByExpression.class)' has location not matching its contents: contains class RepartitionByExpression\n",
       "error: error while loading ObjectProducer, class file '/opt/spark-2.3.2-bin-hadoop2.7/jars/spark-catalyst_2.11-2.3.2.jar(org/apache/spark/sql/catalyst/plans/logical/ObjectProducer.class)' has location not matching its contents: contains class ObjectProducer\n",
       "error: error while loading FunctionUtils, class file '/opt/spark-2.3.2-bin-hadoop2.7/jars/spark-catalyst_2.11-2.3.2.jar(org/apache/spark/sql/catalyst/plans/logical/FunctionUtils.class)' has location not matching its contents: contains class FunctionUtils\n",
       "error: error while loading FlatMapGroupsInR, class file '/opt/spark-2.3.2-bin-hadoop2.7/jars/spark-catalyst_2.11-2.3.2.jar(org/apache/spark/sql/catalyst/plans/logical/FlatMapGroupsInR.class)' has location not matching its contents: contains class FlatMapGroupsInR\n",
       "error: error while loading Union, class file '/opt/spark-2.3.2-bin-hadoop2.7/jars/spark-catalyst_2.11-2.3.2.jar(org/apache/spark/sql/catalyst/plans/logical/Union.class)' has location not matching its contents: contains class Union\n",
       "error: error while loading LeafNode, class file '/opt/spark-2.3.2-bin-hadoop2.7/jars/spark-catalyst_2.11-2.3.2.jar(org/apache/spark/sql/catalyst/plans/logical/LeafNode.class)' has location not matching its contents: contains class LeafNode\n",
       "error: error while loading SerializeFromObject, class file '/opt/spark-2.3.2-bin-hadoop2.7/jars/spark-catalyst_2.11-2.3.2.jar(org/apache/spark/sql/catalyst/plans/logical/SerializeFromObject.class)' has location not matching its contents: contains class SerializeFromObject\n",
       "error: error while loading Command, class file '/opt/spark-2.3.2-bin-hadoop2.7/jars/spark-catalyst_2.11-2.3.2.jar(org/apache/spark/sql/catalyst/plans/logical/Command.class)' has location not matching its contents: contains class Command\n",
       "error: error while loading QueryPlanConstraints, class file '/opt/spark-2.3.2-bin-hadoop2.7/jars/spark-catalyst_2.11-2.3.2.jar(org/apache/spark/sql/catalyst/plans/logical/QueryPlanConstraints.class)' has location not matching its contents: contains class QueryPlanConstraints\n",
       "error: error while loading LogicalPlanVisitor, class file '/opt/spark-2.3.2-bin-hadoop2.7/jars/spark-catalyst_2.11-2.3.2.jar(org/apache/spark/sql/catalyst/plans/logical/LogicalPlanVisitor.class)' has location not matching its contents: contains class LogicalPlanVisitor\n",
       "error: error while loading CoGroup, class file '/opt/spark-2.3.2-bin-hadoop2.7/jars/spark-catalyst_2.11-2.3.2.jar(org/apache/spark/sql/catalyst/plans/logical/CoGroup.class)' has location not matching its contents: contains class CoGroup\n",
       "error: error while loading LocalRelation, class file '/opt/spark-2.3.2-bin-hadoop2.7/jars/spark-catalyst_2.11-2.3.2.jar(org/apache/spark/sql/catalyst/plans/logical/LocalRelation.class)' has location not matching its contents: contains class LocalRelation\n",
       "error: error while loading View, class file '/opt/spark-2.3.2-bin-hadoop2.7/jars/spark-catalyst_2.11-2.3.2.jar(org/apache/spark/sql/catalyst/plans/logical/View.class)' has location not matching its contents: contains class View\n",
       "error: error while loading RepartitionOperation, class file '/opt/spark-2.3.2-bin-hadoop2.7/jars/spark-catalyst_2.11-2.3.2.jar(org/apache/spark/sql/catalyst/plans/logical/RepartitionOperation.class)' has location not matching its contents: contains class RepartitionOperation\n",
       "error: error while loading Aggregate, class file '/opt/spark-2.3.2-bin-hadoop2.7/jars/spark-catalyst_2.11-2.3.2.jar(org/apache/spark/sql/catalyst/plans/logical/Aggregate.class)' has location not matching its contents: contains class Aggregate\n",
       "error: error while loading Sort, class file '/opt/spark-2.3.2-bin-hadoop2.7/jars/spark-catalyst_2.11-2.3.2.jar(org/apache/spark/sql/catalyst/plans/logical/Sort.class)' has location not matching its contents: contains class Sort\n",
       "error: error while loading Repartition, class file '/opt/spark-2.3.2-bin-hadoop2.7/jars/spark-catalyst_2.11-2.3.2.jar(org/apache/spark/sql/catalyst/plans/logical/Repartition.class)' has location not matching its contents: contains class Repartition\n",
       "error: error while loading GroupingSets, class file '/opt/spark-2.3.2-bin-hadoop2.7/jars/spark-catalyst_2.11-2.3.2.jar(org/apache/spark/sql/catalyst/plans/logical/GroupingSets.class)' has location not matching its contents: contains class GroupingSets\n",
       "error: error while loading WithWindowDefinition, class file '/opt/spark-2.3.2-bin-hadoop2.7/jars/spark-catalyst_2.11-2.3.2.jar(org/apache/spark/sql/catalyst/plans/logical/WithWindowDefinition.class)' has location not matching its contents: contains class WithWindowDefinition\n",
       "error: error while loading Expand, class file '/opt/spark-2.3.2-bin-hadoop2.7/jars/spark-catalyst_2.11-2.3.2.jar(org/apache/spark/sql/catalyst/plans/logical/Expand.class)' has location not matching its contents: contains class Expand\n",
       "error: error while loading UnaryNode, class file '/opt/spark-2.3.2-bin-hadoop2.7/jars/spark-catalyst_2.11-2.3.2.jar(org/apache/spark/sql/catalyst/plans/logical/UnaryNode.class)' has location not matching its contents: contains class UnaryNode\n",
       "error: error while loading Subquery, class file '/opt/spark-2.3.2-bin-hadoop2.7/jars/spark-catalyst_2.11-2.3.2.jar(org/apache/spark/sql/catalyst/plans/logical/Subquery.class)' has location not matching its contents: contains class Subquery\n",
       "error: error while loading ReturnAnswer, class file '/opt/spark-2.3.2-bin-hadoop2.7/jars/spark-catalyst_2.11-2.3.2.jar(org/apache/spark/sql/catalyst/plans/logical/ReturnAnswer.class)' has location not matching its contents: contains class ReturnAnswer\n",
       "error: error while loading MapElements, class file '/opt/spark-2.3.2-bin-hadoop2.7/jars/spark-catalyst_2.11-2.3.2.jar(org/apache/spark/sql/catalyst/plans/logical/MapElements.class)' has location not matching its contents: contains class MapElements\n",
       "error: error while loading FlatMapGroupsWithState, class file '/opt/spark-2.3.2-bin-hadoop2.7/jars/spark-catalyst_2.11-2.3.2.jar(org/apache/spark/sql/catalyst/plans/logical/FlatMapGroupsWithState.class)' has location not matching its contents: contains class FlatMapGroupsWithState\n",
       "error: error while loading Range, class file '/opt/spark-2.3.2-bin-hadoop2.7/jars/spark-catalyst_2.11-2.3.2.jar(org/apache/spark/sql/catalyst/plans/logical/Range.class)' has location not matching its contents: contains class Range\n",
       "error: error while loading Limit, class file '/opt/spark-2.3.2-bin-hadoop2.7/jars/spark-catalyst_2.11-2.3.2.jar(org/apache/spark/sql/catalyst/plans/logical/Limit.class)' has location not matching its contents: contains class Limit\n",
       "error: error while loading GlobalLimit, class file '/opt/spark-2.3.2-bin-hadoop2.7/jars/spark-catalyst_2.11-2.3.2.jar(org/apache/spark/sql/catalyst/plans/logical/GlobalLimit.class)' has location not matching its contents: contains class GlobalLimit\n",
       "error: error while loading Deduplicate, class file '/opt/spark-2.3.2-bin-hadoop2.7/jars/spark-catalyst_2.11-2.3.2.jar(org/apache/spark/sql/catalyst/plans/logical/Deduplicate.class)' has location not matching its contents: contains class Deduplicate\n",
       "error: error while loading Project, class file '/opt/spark-2.3.2-bin-hadoop2.7/jars/spark-catalyst_2.11-2.3.2.jar(org/apache/spark/sql/catalyst/plans/logical/Project.class)' has location not matching its contents: contains class Project\n",
       "error: error while loading BinaryNode, class file '/opt/spark-2.3.2-bin-hadoop2.7/jars/spark-catalyst_2.11-2.3.2.jar(org/apache/spark/sql/catalyst/plans/logical/BinaryNode.class)' has location not matching its contents: contains class BinaryNode\n",
       "error: error while loading Except, class file '/opt/spark-2.3.2-bin-hadoop2.7/jars/spark-catalyst_2.11-2.3.2.jar(org/apache/spark/sql/catalyst/plans/logical/Except.class)' has location not matching its contents: contains class Except\n",
       "error: error while loading NoTimeout, class file '/opt/spark-2.3.2-bin-hadoop2.7/jars/spark-catalyst_2.11-2.3.2.jar(org/apache/spark/sql/catalyst/plans/logical/NoTimeout.class)' has location not matching its contents: contains class NoTimeout\n",
       "error: error while loading CatalystSerde, class file '/opt/spark-2.3.2-bin-hadoop2.7/jars/spark-catalyst_2.11-2.3.2.jar(org/apache/spark/sql/catalyst/plans/logical/CatalystSerde.class)' has location not matching its contents: contains class CatalystSerde\n",
       "error: error while loading ProcessingTime, class file '/opt/spark-2.3.2-bin-hadoop2.7/jars/spark-sql_2.11-2.3.2.jar(org/apache/spark/sql/streaming/ProcessingTime.class)' has location not matching its contents: contains class ProcessingTime\n",
       "error: error while loading StateOperatorProgress, class file '/opt/spark-2.3.2-bin-hadoop2.7/jars/spark-sql_2.11-2.3.2.jar(org/apache/spark/sql/streaming/StateOperatorProgress.class)' has location not matching its contents: contains class StateOperatorProgress\n",
       "error: error while loading DataStreamWriter, class file '/opt/spark-2.3.2-bin-hadoop2.7/jars/spark-sql_2.11-2.3.2.jar(org/apache/spark/sql/streaming/DataStreamWriter.class)' has location not matching its contents: contains class DataStreamWriter\n",
       "error: error while loading Trigger, class file '/opt/spark-2.3.2-bin-hadoop2.7/jars/spark-sql_2.11-2.3.2.jar(org/apache/spark/sql/streaming/Trigger.class)' has location not matching its contents: contains class Trigger\n",
       "error: error while loading StreamingQueryException, class file '/opt/spark-2.3.2-bin-hadoop2.7/jars/spark-sql_2.11-2.3.2.jar(org/apache/spark/sql/streaming/StreamingQueryException.class)' has location not matching its contents: contains class StreamingQueryException\n",
       "error: error while loading SourceProgress, class file '/opt/spark-2.3.2-bin-hadoop2.7/jars/spark-sql_2.11-2.3.2.jar(org/apache/spark/sql/streaming/SourceProgress.class)' has location not matching its contents: contains class SourceProgress\n",
       "error: error while loading StreamingQueryProgress, class file '/opt/spark-2.3.2-bin-hadoop2.7/jars/spark-sql_2.11-2.3.2.jar(org/apache/spark/sql/streaming/StreamingQueryProgress.class)' has location not matching its contents: contains class StreamingQueryProgress\n",
       "error: error while loading SinkProgress, class file '/opt/spark-2.3.2-bin-hadoop2.7/jars/spark-sql_2.11-2.3.2.jar(org/apache/spark/sql/streaming/SinkProgress.class)' has location not matching its contents: contains class SinkProgress\n",
       "error: error while loading StreamingQueryListener, class file '/opt/spark-2.3.2-bin-hadoop2.7/jars/spark-sql_2.11-2.3.2.jar(org/apache/spark/sql/streaming/StreamingQueryListener.class)' has location not matching its contents: contains class StreamingQueryListener\n",
       "error: error while loading package, class file '/opt/spark-2.3.2-bin-hadoop2.7/jars/spark-sql_2.11-2.3.2.jar(org/apache/spark/sql/execution/streaming/state/package.class)' has location not matching its contents: contains package object state\n",
       "error: error while loading StateStoreCoordinatorRef, class file '/opt/spark-2.3.2-bin-hadoop2.7/jars/spark-sql_2.11-2.3.2.jar(org/apache/spark/sql/execution/streaming/state/StateStoreCoordinatorRef.class)' has location not matching its contents: contains class StateStoreCoordinatorRef\n",
       "error: error while loading StreamingQueryListenerBus, class file '/opt/spark-2.3.2-bin-hadoop2.7/jars/spark-sql_2.11-2.3.2.jar(org/apache/spark/sql/execution/streaming/StreamingQueryListenerBus.class)' has location not matching its contents: contains class StreamingQueryListenerBus\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "%%dataframe [arguments]\n",
       "DATAFRAME_CODE\n",
       "\n",
       "DATAFRAME_CODE can be any numbered lines of code, as long as the\n",
       "last line is a reference to a variable which is a DataFrame.\n",
       "    Option    Description                       \n",
       "------    -----------                       \n",
       "--limit   The number of records to return   \n",
       "            (default: 10)                   \n",
       "--output  The type of the output: html, csv,\n",
       "            json (default: html)            \n"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table><tr><th>key</th><th>value</th></tr><tr><td>1</td><td>1</td></tr><tr><td>2</td><td>2</td></tr><tr><td>3</td><td>3</td></tr><tr><td>4</td><td>4</td></tr><tr><td>5</td><td>5</td></tr><tr><td>6</td><td>6</td></tr><tr><td>7</td><td>7</td></tr><tr><td>8</td><td>8</td></tr><tr><td>9</td><td>9</td></tr><tr><td>10</td><td>10</td></tr></table>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%dataframe\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can specify the `--output` argument to change the output type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "key,value\n",
       "1,1\n",
       "2,2\n",
       "3,3\n",
       "4,4\n",
       "5,5\n",
       "6,6\n",
       "7,7\n",
       "8,8\n",
       "9,9\n",
       "10,10"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%dataframe --output=csv\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is also an option to limit the number of records returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table><tr><th>key</th><th>value</th></tr><tr><td>1</td><td>1</td></tr><tr><td>2</td><td>2</td></tr><tr><td>3</td><td>3</td></tr></table>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%dataframe --limit=3\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### %%Html<a name=\"html\"></a><span style=\"float: right; font-size: 0.5em\"><a href=\"#top\">Top</a></span>\n",
    "The `%%HTML` magic allows you to return HTML."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p>\n",
       "Hello, <strong>Magics</strong>!\n",
       "</p>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%html\n",
    "<p>\n",
    "Hello, <strong>Magics</strong>!\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### %%JavaScript<a name=\"javascript\"></a><span style=\"float: right; font-size: 0.5em\"><a href=\"#top\">Top</a></span>\n",
    "The `%%JavaScript` magic allows to return JavaScript. The JavaScript code will run in the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "alert(\"Hello, Magics!\")"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%JavaScript\n",
    "alert(\"Hello, Magics!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### %%PySpark<a name=\"pyspark\"></a><span style=\"float: right; font-size: 0.5em\"><a href=\"#top\">Top</a></span>\n",
    "The `%%PySpark` exposes an environment with and a python interpreter and a shared `SparkContext`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Magic PySpark failed to execute with error: \n",
       "null"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%PySpark\n",
    "from operator import add\n",
    "print(sc.parallelize(range(1, 100)).reduce(add))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### %%SparkR<a name=\"sparkr\"></a><span style=\"float: right; font-size: 0.5em\"><a href=\"#top\">Top</a></span>\n",
    "The `%%SparkR` exposes an environment with and an R interpreter and a shared `SparkContext`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Magic SparkR failed to execute with error: \n",
       "null"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%SparkR\n",
    "df <- createDataFrame(spark, faithful)\n",
    "head(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### %%SparkSQL<a name=\"sparksql\"></a><span style=\"float: right; font-size: 0.5em\"><a href=\"#top\">Top</a></span>\n",
    "The `%%SparkSQL` magic allows for SQL queries to be performed against tables saved in spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sqlc = org.apache.spark.sql.SparkSession@3e040e2e\n",
       "defined class Record\n",
       "df = [key: string, value: int]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "warning: there was one deprecation warning; re-run with -deprecation for details\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[key: string, value: int]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val sqlc = spark\n",
    "import sqlc.implicits._\n",
    "case class Record(key: String, value: Int)\n",
    "val df = sc.parallelize(1 to 10).map(x => Record(x.toString, x)).toDF()\n",
    "df.registerTempTable(\"MYTABLE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "+---+-----+\n",
       "|key|value|\n",
       "+---+-----+\n",
       "|  6|    6|\n",
       "|  7|    7|\n",
       "|  8|    8|\n",
       "|  9|    9|\n",
       "| 10|   10|\n",
       "+---+-----+\n",
       "\n"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%SQL\n",
    "SELECT * FROM MYTABLE WHERE value >= 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "+---+-----+\n",
       "|key|value|\n",
       "+---+-----+\n",
       "|  4|    4|\n",
       "|  5|    5|\n",
       "|  6|    6|\n",
       "|  7|    7|\n",
       "|  8|    8|\n",
       "|  9|    9|\n",
       "| 10|   10|\n",
       "+---+-----+\n",
       "\n"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%SQL\n",
    "SELECT * FROM MYTABLE WHERE value >= 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
